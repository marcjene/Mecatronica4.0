{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/top_ML.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenido\n",
    "\n",
    "1. Regresión lineal\n",
    "2. Regresión lineal con sklearn\n",
    "3. Métricas evaluación\n",
    "5. Visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regresión lineal\n",
    "\n",
    "**Notación para el modelo de regresión lineal**\n",
    "\n",
    "En un modelo lineal, tendremos un prámetro $\\textbf{y}$ que depende de manera lineal de varios covariantes $\\textbf{x}_i$:\n",
    "\n",
    "$$ \\textbf{y}  =  a_1 \\textbf{x}_1  + \\dots + a_m \\textbf{x}_{m} $$\n",
    "\n",
    "Los términos $a_i$ serán los *parametros* del modelo o *coeficientes*.\n",
    "\n",
    "Si lo escribimos de forma matricial:\n",
    "\n",
    "$$ \\textbf{y}  = X \\textbf{w}$$\n",
    "\n",
    "donde $$ \\textbf{y} = \\left( \\begin{array}{c} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{array} \\right), \n",
    " X = \\left( \\begin{array}{c} x_{11}  \\dots x_{1m} \\\\ x_{21}  \\dots x_{2m}\\\\ \\vdots \\\\ x_{n1}  \\dots x_{nm} \\end{array} \\right),\n",
    " \\textbf{w} = \\left( \\begin{array}{c} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_m \\end{array} \\right) $$\n",
    " \n",
    "En un modelo de regresión lineal que solo dependa de una variable, tendremos:\n",
    "\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$\n",
    "\n",
    "Con un parámetro $a_0$ llamado constante o corte con el eje de ordenadas.\n",
    "\n",
    "Si tenemos una regresión multivariable, tendremos:\n",
    "$$ \\textbf{y} = a_1 \\textbf{x}_1 + \\dots + a_m \\textbf{x}_m = X \\textbf{w} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver algun ejemplo de manera gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n",
    "%matplotlib inline \n",
    "plt.rc('font', size=12) \n",
    "plt.rc('figure', figsize = (12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si creamos un conjunto de datos \"aleatorios\", y los graficamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X1 = np.random.randn(300, 2)  # Aleatorios siguiendo una Gauss\n",
    "A = np.array([[0.6, .4], [.4, 0.6]]) # Aplicació lineal per fer-la \"una funció lineal\"\n",
    "X2 = np.dot(X1, A)\n",
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Está claaro que hay una cierta correlación entre ellos que podríamos ver con este modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model=[0+1*x for x in np.arange(-2,3)] # un possible model\n",
    "\n",
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n",
    "plt.plot(np.arange(-2,3), model,'r'); \n",
    "for xi, yi in zip(X2[:, 0],X2[:, 1]):\n",
    "    plt.plot([xi]*2, [yi, 0+1*xi], \"k:\") # ilusrtrar errores\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero, ¿Cuál es el modelo que mejor se ajusta a estos datos? \n",
    "\n",
    "¿Cómo encontramos los parámetros o coeficientes de la siguiente ecuación?\n",
    "\n",
    "$$ \\textbf{y}  =  a_0+ a_1 \\textbf{x}_1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(X2[:, 0], X2[:, 1], \"o\", alpha=0.3);\n",
    "#más modelos!!\n",
    "model1=[0+1*x for x in np.arange(-2,3)]\n",
    "model2=[0.3+0.8*x for x in np.arange(-2,3)]\n",
    "model3=[0-1.2*x for x in np.arange(-2,3)]\n",
    "plt.plot(np.arange(-2,3), model1,'r')\n",
    "plt.plot(np.arange(-2,3), model2,'g')\n",
    "plt.plot(np.arange(-2,3), model3,'y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo siempre será minimizar la suma del cuadrado de la distancia entro los puntos reales y el valor de la funcióm\n",
    "\n",
    "Si tenemos los datos $(\\textbf{x},\\textbf{y})$, queremos minimizar:\n",
    "\n",
    "$$ ||a_0 + a_1 \\textbf{x} -  \\textbf{y} ||^2_2 = \\sum_{j=1}^n (a_0+a_1 x_{j} -  y_j )^2,$$ \n",
    "\n",
    "Esta expresión, se conoce como **sum of squared errors of prediction (SSE)**.\n",
    "\n",
    "La manera más fàcil de encontrar estos dos parámetros es usando el algoritmo OLS (*Ordinary Least Squares*)\n",
    "\n",
    "$$\\textbf{y} = a_0+a_1 \\textbf{x}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1 OLS\n",
    "\n",
    "Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([2.2, 4.3, 5.1, 5.8, 6.4, 8.0])\n",
    "x_train = x.reshape(1,-1) \n",
    "\n",
    "y = np.array([0.4, 10.1, 14.0, 10.9, 15.4, 18.5])\n",
    "y_train = y.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizar las distancias\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "sse = lambda b, x, y: np.sum((y - b[0] - b[1]*x) ** 2) # Guardar la SSE\n",
    "\n",
    "b0,b1 = fmin(sse, [0,1], args=(x,y)); # Minimizar usando 0 y 1 como valores iniciales de los coeficientes\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10], alpha=0.8) # linia regresión\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, b0+b1*xi], \"k:\") # ilusrtrar errores\n",
    "plt.xlim(2, 9); plt.ylim(0, 20)\n",
    "\n",
    "print('\\nSSE = ', np.sum((y - b0 - b1*x) ** 2))\n",
    "print('\\nModelo obtenido:\\n',round(b0,2),'+',round(b1,2),'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podríamos minimizar otros valores como  **suma del valor absoluto de las diferencias**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimizar valores absolutos\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "sabs = lambda b, x, y: np.sum(np.abs(y - b[0] - b[1]*x)) \n",
    "\n",
    "b0,b1 = fmin(sabs, [0,1], args=(x,y)) # Minimizar valores absolutos ahora\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot([0,10], [b0, b0+b1*10]) # linia regresión\n",
    "for xi, yi in zip(x,y):\n",
    "    plt.plot([xi]*2, [yi, b0+b1*xi], \"k:\") # ilusrtrar errores\n",
    "plt.xlim(2, 9); plt.ylim(0, 20) \n",
    "\n",
    "print('\\nSSE = ', np.sum((y - b0 - b1*x) ** 2))\n",
    "print('Valores Absolutos = ', np.sum(np.abs(y - b0 - b1*x)))\n",
    "print('\\nModelo obtenido:\\n',round(b0,2),'+',round(b1,2),'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, penalizamos menos los valores lejanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ventajas OLS\n",
    "\n",
    "+ Fácil de calcular computacionalmente cuando son datasets pequeños. Para conjuntos más grandes el cálculo de una inversa provoca un aumento del tiempo de cómputo.\n",
    "+ Fácil de interpretar\n",
    "\n",
    "Y el modelo obtenido es:\n",
    "\n",
    "$$\\widehat{\\textbf{y}} = \\widehat{a}_0+\\widehat{a}_1 \\textbf{x}$$\n",
    "\n",
    "Los sombreros indican que són valores estimados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Regresión lineal con Sklearn\n",
    "\n",
    "Por suerte no tenemos que desarrolar estos algoritmos nosotros des de cero. Para esto están las librerías de machine learning ya hechas!!\n",
    "\n",
    "Por ejemplo, veamos como de fácil es crear un modelo de regresión lineal en Sklearn, cargando un dataset de ejemplo de la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Cargar dataset ejemplo\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para ejecutar el modelo, es así de fácil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Crear modelo regresion lineal\n",
    "regr = LinearRegression()\n",
    "\n",
    "# Entrear\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Obtener modelo\n",
    "print('a1 es: \\n', regr.coef_)\n",
    "print('a0 es: \\n', regr.intercept_)\n",
    "\n",
    "# Plot \n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot([-0.1,0.1], [b0, regr.intercept_+regr.coef_[0]*0.1]) # linia regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une vez obtenido el modelo también podemos hacer predicciones directamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones\n",
    "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
    "\n",
    "# Plot \n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Métricas de evaluación\n",
    "\n",
    "Se puede evaluar el modelo obtenido calculando el **mean squared error** ($MSE$) y el **coeficiente de determinación** $R^2$\n",
    "\n",
    "El MSE, se calcula como:\n",
    "\n",
    "$$MSE=\\frac{1}{n} \\sum_{i=1}^n (\\widehat{y}^i-y^i)^2,$$ \n",
    "\n",
    "El coeficiente $R^2$ se define \n",
    "$$(1 - \\textbf{u}/\\textbf{v})$$, donde $\\textbf{u}$ es la suma de los cuadrados de los errores: $$\\textbf{u}=\\sum (\\textbf{y} - \\widehat{\\textbf{y}} )^2$$ \n",
    "donde ${\\textbf{y}}$ son los valores observados y  $\\widehat{\\textbf{y}}$ los valores de la predicción.\n",
    "\n",
    "Y $\\textbf{v}$ es el total de la suma de los cuadrado: $$\\textbf{v}=\\sum (\\textbf{y} - \\bar{\\textbf{y}})^2,$$ donde $\\bar{\\textbf{y}}$ es la media de los datos observados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Sklearn, podríamos hacer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print('Mean squared error (MSE): %.2f'\n",
    "      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coeficiente de determinación: %.2f'\n",
    "      % r2_score(diabetes_y_test, diabetes_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualización\n",
    "\n",
    "Podemos usar la función ``lmplot()`` de Seaborn para visualizar relaciones lineales de datasets multidimensionales. El input debe ser en *Pandas* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1:  Macroeconomic dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Read data\n",
    "df = pd.read_csv('http://vincentarelbundock.github.io/Rdatasets/csv/datasets/longley.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macroeconomic data from 1947 to 1962.\n",
    "\n",
    "Queremos predecir  ('Employed') como respuesta $\\textbf{y}$ usando ('GNP') como predictor $\\textbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"GNP\", \"Employed\", df, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos\n",
    "+ los puntos observador en scatterplot\n",
    "+ La línia de regresión obtenida con un intervalo de confianza de un 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"GNP\", \"Population\", df, aspect=2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(\"Armed.Forces\", \"Unemployed\", df, aspect=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay partes poco \"lineales\".\n",
    "\n",
    "Para ello podemos usar la regresión polinomial.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
